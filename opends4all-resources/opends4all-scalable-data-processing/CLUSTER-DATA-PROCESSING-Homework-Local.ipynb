{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework_no_Sol_Local_New_Data.ipynb","provenance":[{"file_id":"1V4kiujaAnm3hHMomSK1eDz5jQjdKxmWU","timestamp":1571083042642}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qVMCjEV1IEC7","colab_type":"text"},"source":["#Homework: Spark SQL\n","\n","In this homework you will gain a mastery of using Spark SQL. The homework can be run locally or on an EMR cluster.  The current version is for running locally.  \n","\n","The goal of the homework will be to create a training dataset for a Random Forest Machine learning model. The training data set will contain the monthly number of employees hired by companies in `linkedin.json` and their corresponding closing stock prices over a 10+ year period (1970-2018 `stock_prices.csv`). We will try and predict, based on this data, if the company will have a positive or negative growth in stock in the first quarter of the next year. Who's ready to make some money?\n","\n","## Notes\n","Before we begin here are some important notes to keep in mind,\n","\n","1. You are **required** to use Spark SQL queries to handle the data in the assignment. Mastering SQL is more beneficial than being able to use Spark commands (functions) as it will show up in more areas of programming and data science/analytics than just Spark. Use the following [function list](https://spark.apache.org/docs/latest/api/sql/index.html#) to see all the SQL functions avaliable in Spark.\n","\n","2. There are portions of this homework that are _very_ challenging. \n"]},{"cell_type":"code","metadata":{"id":"pvkEbVaaAQ1e","colab_type":"code","colab":{}},"source":["%%capture\n","!apt update\n","!apt install gcc python-dev libkrb5-dev\n","!pip install sparkmagic\n","!pip install pyspark"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IAvEMpzqqHeY","colab_type":"code","colab":{}},"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import *\n","import pyspark.sql.functions as F\n","from pyspark.sql import SQLContext\n","import json\n","import urllib.request\n","\n","from datetime import datetime\n","\n","try:\n","    if(spark == None):\n","        spark = SparkSession.builder.appName('Graphs').getOrCreate()\n","        sqlContext=SQLContext(spark)\n","        \n","except NameError:\n","    spark = SparkSession.builder.appName('Graphs').getOrCreate()\n","    sqlContext=SQLContext(spark)\n","        \n","from pyspark.sql.types import *"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nf_ADEXnIK0b","colab_type":"text"},"source":["## Step 1: Data Cleaning and Shaping\n","\n","When used for single machine like Colab, you should mount Google Drive to Colab and visit the data file locally. \n","\n","If used remotely, please refer to the 'remote' version of the notebook. For remote version, the data you will use is stored in an S3 bucket, a cloud storage service. You now need to download it onto the nodes of your [EMR cluster](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html). \n","\n","### Step 1.1: The Stupendous Schema\n","\n","When loading data, Spark will try to infer the structure. This process is faulty because it will sometimes infer the type incorrectly. JSON documents, like the one we will use, can have nested types, such as: arrays, arrays of dictionaries, dictionaries of dictionaries, etc. Spark's ability to determine these nested types is not reliable, thus you will define a schema for `linkedin.json`.\n","\n","A schema is a description of the structure of data. You will be defining an explicit schema for `linkedin.json`. In Spark, schemas are defined using a `StructType` object. This is a collection of data types, termed `StructField`s, that specify the structure and variable type of each component of the dataset. For example, suppose we have the following simple JSON object,\n","\n","\n","```\n","{\n"," \"student_name\": \"Leonardo Murri\",\n"," \"GPA\": 1.4,\n"," \"courses\": [\n","    {\"department\": \"Computer and Information Science\",\n","     \"course_id\": \"CIS 545\",\n","     \"semester\": \"Fall 2018\"},\n","    {\"department\": \"Computer and Information Science\",\n","     \"course_id\": \"CIS 520\",\n","     \"semester\": \"Fall 2018\"},\n","    {\"department\": \"Electrical and Systems Engineering\",\n","     \"course_id\": \"ESE 650\",\n","     \"semester\": \"Spring 2018\"}\n"," ],\n"," \"grad_year\": 2019\n"," }\n","```\n","\n","We would define its schema as follows,\n","\n","```       \n","schema = StructType([\n","           StructField(\"student_name\", StringType(), nullable=True),\n","           StructField(\"GPA\", FloatType(), nullable=True),\n","           StructField(\"courses\", ArrayType(\n","                StructType([\n","                  StructField(\"department\", StringType(), nullable=True),\n","                  StructField(\"course_id\", StringType(), nullable=True),\n","                  StructField(\"semester\", StringType(), nullable=True)\n","                ])\n","           ), nullable=True),\n","           StructField(\"grad_year\", IntegerType(), nullable=True)\n","         ])\n","```\n","\n","\n","Each `StructField` has the following structure: `(name, type, nullable)`. The `nullable` flag defines that the specified field may be empty. Your first task is to define the `schema` of `linkedin.json`. \n","\n","_Note_: In `linkedin.json` the field `specilities` is spelled incorrectly. This is **not** a typo. \n"]},{"cell_type":"code","metadata":{"id":"pL-Ps4KWIJ9e","colab_type":"code","colab":{}},"source":["# TODO: Define [linkedin.json] schema\n","# YOUR CODE HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Su604X9ggc2","colab_type":"text"},"source":["### Step 1.2: The Laudable Loading\n","\n","Load the `linkedin.json` dataset into a Spark dataframe (sdf) called `raw_data_sdf`. If you have constructed `schema` correctly `spark.read.json()` will read in the dataset. ***You do not need to edit this cell***."]},{"cell_type":"code","metadata":{"id":"wNsZhq-pDGgB","colab_type":"code","outputId":"ee22c813-a542-4262-ba43-2c7782c523d8","executionInfo":{"status":"ok","timestamp":1582169581096,"user_tz":300,"elapsed":21575,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ji-KW2sAiB6r","colab_type":"code","colab":{}},"source":["raw_data_sdf = spark.read.json('/content/drive/My Drive/Colab Notebooks/test_data_10000.json', schema=schema)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FkW0CN1VT3kd","colab_type":"code","outputId":"12d38aba-59df-40db-ccb0-17a4f0e240d7","executionInfo":{"status":"ok","timestamp":1582171462196,"user_tz":300,"elapsed":526,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["raw_data_sdf.show(10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|                 _id|           education|               group|                name|            locality|              skills|            industry|interval|          experience|             summary|           interests|       overview_html|         specilities|            homepage|              honors|                 url|           also_view|              events|\n","+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|ichsrrdhpxlojntri...|[[2007, Computer ...|                null|   [Klenin, Ignacio]|   Eskisehir, Turkey|[{\"value\":\"Cellul...|           Animasyon|    null|[[UBS, Executive ...|An experienced ge...|Reading books, wa...|                null|Areas Of Practice...|                null|[{\"value\":\"-Talen...|ftsycpiuiasaecavd...|[[in-kaliouby, ht...|                  []|\n","|sxaybhrpeceeanwwq...|[[1990,, 1995, Th...|[[Entertainment T...|       [Oprea, Adam]|York, United Kingdom|                  []|     Pharmaceuticals|    null|[[Spencer J Lloyd...|1 - Strategic Man...|Travel, Live Musi...|<dl id=\"overview\"...|redacción, comuni...|                null|                  []|zramixfzvfpoiysoa...|                  []|[[A.R.M. Autoleas...|\n","|cfogybqdnddowlhca...|[[1970, Ciencias ...|[[Denim People's ...|      [London, Ajay]|Greater Nashville...|                  []|           Asigurări|    null|[[AIMS Accountant...|An experienced an...|Sales and busines...|<dl id=\"overview\"...|Applied microecon...|{'Personal Websit...|                  []|ftcisrobxsxkayhot...|[[pub-evan-willem...|[[Madix, Inc, Bec...|\n","|vzcoxvvnuarepgqxm...|[[1976,, 1980, Un...|[[BRASIL: VAGAS E...|[Kilbourne, CPSM,...|Madison, Wisconsi...|                  []|    Higher Education|    null|[[Deshel, Thought...|Passionate about ...|Reading Magzines,...|<dl id=\"overview\"...|DSP (speech, audi...|{'Blog': ['http:/...|                  []|xgjwidohcoapijxrz...|[[pub-sanjay-bali...|[[Safeway, Tolt S...|\n","|qcocuvfhzactuygqs...|[[2000, Mathemati...|[[A&I - Accoglien...|  [Grando, ADAMARIO]|Santa Monica, Cal...|                  []|Compagnie aérienn...|    null|[[The Best Match ...|20+ years of cons...|Networks, algorit...|<dl id=\"overview\"...|Territory Managem...|                null|                  []|podsgvxoxjtqytuto...|[[pub-rakhmat-han...|                  []|\n","|zclxbuwmpxdfyjqod...|[[1982,, 1985, TA...|[[AngloINFO Luxem...|      [Mazzan, Ægir]|Région de Berne, ...|                  []|Företagsrådgivnin...|    null|[[Inland Die Cast...|Senior marketing ...|Software architec...|                null|Transport, Energy...|{'Equinoxia': ['h...|                  []|bjzrawqtzzimbnfki...|[[pub-cassidy-alo...|[[Grupo Saint Gob...|\n","|hfabkdaosboiuahnb...|[[1993, Tourism M...|[[IIHEM Alumni As...|   [HTX, Alessandra]|           Финляндия|[{\"value\":\"Google...|Politieke organis...|    null|[[Waanders In de ...|Accomplished Seni...|      New technology|<dl id=\"overview\"...|Extensive project...|{'Portfolio': ['h...|                  []|mhomrevpmexgahplj...|                  []|[[Caixa Economica...|\n","|gdngchsbqmiangfuy...|[[2010, Financial...|[[Affordable Hous...|    [Tegerdine, Ain]|Milwaukee, Wisconsin|[{\"value\":\"Event ...|      Mediaproductie|    null|[[Xebia, CTO Deve...|I am a highly mot...|pharmaceutical de...|<dl id=\"overview\"...|Mobile phone soft...|                null|                  []|wltetjmkamjtcalgs...|[[pub-adina-milit...|[[Saudi Aramco, S...|\n","|blzhknuzppzoowgrh...|[[2003, Masters i...|[[American League...|    [Zakout, Adeena]|Greater Detroit Area|[{\"value\":\"Consul...|Marketing and Adv...|    null|[[NVIDIA, Senior ...|Entrepreneurial p...|• Reservoir geolo...|<dl id=\"overview\"...|Communication & B...|{'Personal Websit...|                  []|fxytkwxnzckrdtkoi...|[[in-adityadsharm...|                  []|\n","|uiehloqlgpryzqwbb...|[[2009, CCNA, MCS...|[[The University ...|  [Furman, Madalina]|Alba County, Romania|[{\"value\":\"ETL\"},...|Traduzione e loca...|    null|[[Dirk Bikkemberg...|Specialties: Inte...|Motorbikes, Histo...|                null|SEO, Product Deve...|{'The Learning Co...|                  []|tafqulhcubhqmfdxd...|[[pub-rajiv-willi...|[[Liones, TU Delf...|\n","+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","only showing top 10 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zCmFnfgCwv-V","colab_type":"code","outputId":"41c2d096-daeb-48ab-a8be-35c0250e6c10","executionInfo":{"status":"ok","timestamp":1582171463428,"user_tz":300,"elapsed":618,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["raw_data_sdf.where(F.col('_id').isNull()).count()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"markdown","metadata":{"id":"jMVCVotcE1wv","colab_type":"text"},"source":["The cell below shows how to run SQL commands on Spark tables. Use this as a template for all your SQL queries in this notebook. ***You do not need to edit this cell***."]},{"cell_type":"code","metadata":{"id":"NJSVWeGiEO5c","colab_type":"code","outputId":"969045cb-e092-4f50-9cd0-c956049a7c47","executionInfo":{"status":"ok","timestamp":1582171477319,"user_tz":300,"elapsed":1416,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["# Create SQL-accesible table\n","raw_data_sdf.createOrReplaceTempView(\"raw_data\")\n","\n","# Declare SQL query to be excecuted\n","query = '''SELECT * \n","           FROM raw_data'''\n","\n","# Save the output sdf of spark.sql() as answer_sdf\n","answer_sdf = spark.sql(query)\n","\n","# Display the first 10 rows\n","answer_sdf.show(10)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|                 _id|           education|               group|                name|            locality|              skills|            industry|interval|          experience|             summary|           interests|       overview_html|         specilities|            homepage|              honors|                 url|           also_view|              events|\n","+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|ichsrrdhpxlojntri...|[[2007, Computer ...|                null|   [Klenin, Ignacio]|   Eskisehir, Turkey|[{\"value\":\"Cellul...|           Animasyon|    null|[[UBS, Executive ...|An experienced ge...|Reading books, wa...|                null|Areas Of Practice...|                null|[{\"value\":\"-Talen...|ftsycpiuiasaecavd...|[[in-kaliouby, ht...|                  []|\n","|sxaybhrpeceeanwwq...|[[1990,, 1995, Th...|[[Entertainment T...|       [Oprea, Adam]|York, United Kingdom|                  []|     Pharmaceuticals|    null|[[Spencer J Lloyd...|1 - Strategic Man...|Travel, Live Musi...|<dl id=\"overview\"...|redacción, comuni...|                null|                  []|zramixfzvfpoiysoa...|                  []|[[A.R.M. Autoleas...|\n","|cfogybqdnddowlhca...|[[1970, Ciencias ...|[[Denim People's ...|      [London, Ajay]|Greater Nashville...|                  []|           Asigurări|    null|[[AIMS Accountant...|An experienced an...|Sales and busines...|<dl id=\"overview\"...|Applied microecon...|{'Personal Websit...|                  []|ftcisrobxsxkayhot...|[[pub-evan-willem...|[[Madix, Inc, Bec...|\n","|vzcoxvvnuarepgqxm...|[[1976,, 1980, Un...|[[BRASIL: VAGAS E...|[Kilbourne, CPSM,...|Madison, Wisconsi...|                  []|    Higher Education|    null|[[Deshel, Thought...|Passionate about ...|Reading Magzines,...|<dl id=\"overview\"...|DSP (speech, audi...|{'Blog': ['http:/...|                  []|xgjwidohcoapijxrz...|[[pub-sanjay-bali...|[[Safeway, Tolt S...|\n","|qcocuvfhzactuygqs...|[[2000, Mathemati...|[[A&I - Accoglien...|  [Grando, ADAMARIO]|Santa Monica, Cal...|                  []|Compagnie aérienn...|    null|[[The Best Match ...|20+ years of cons...|Networks, algorit...|<dl id=\"overview\"...|Territory Managem...|                null|                  []|podsgvxoxjtqytuto...|[[pub-rakhmat-han...|                  []|\n","|zclxbuwmpxdfyjqod...|[[1982,, 1985, TA...|[[AngloINFO Luxem...|      [Mazzan, Ægir]|Région de Berne, ...|                  []|Företagsrådgivnin...|    null|[[Inland Die Cast...|Senior marketing ...|Software architec...|                null|Transport, Energy...|{'Equinoxia': ['h...|                  []|bjzrawqtzzimbnfki...|[[pub-cassidy-alo...|[[Grupo Saint Gob...|\n","|hfabkdaosboiuahnb...|[[1993, Tourism M...|[[IIHEM Alumni As...|   [HTX, Alessandra]|           Финляндия|[{\"value\":\"Google...|Politieke organis...|    null|[[Waanders In de ...|Accomplished Seni...|      New technology|<dl id=\"overview\"...|Extensive project...|{'Portfolio': ['h...|                  []|mhomrevpmexgahplj...|                  []|[[Caixa Economica...|\n","|gdngchsbqmiangfuy...|[[2010, Financial...|[[Affordable Hous...|    [Tegerdine, Ain]|Milwaukee, Wisconsin|[{\"value\":\"Event ...|      Mediaproductie|    null|[[Xebia, CTO Deve...|I am a highly mot...|pharmaceutical de...|<dl id=\"overview\"...|Mobile phone soft...|                null|                  []|wltetjmkamjtcalgs...|[[pub-adina-milit...|[[Saudi Aramco, S...|\n","|blzhknuzppzoowgrh...|[[2003, Masters i...|[[American League...|    [Zakout, Adeena]|Greater Detroit Area|[{\"value\":\"Consul...|Marketing and Adv...|    null|[[NVIDIA, Senior ...|Entrepreneurial p...|• Reservoir geolo...|<dl id=\"overview\"...|Communication & B...|{'Personal Websit...|                  []|fxytkwxnzckrdtkoi...|[[in-adityadsharm...|                  []|\n","|uiehloqlgpryzqwbb...|[[2009, CCNA, MCS...|[[The University ...|  [Furman, Madalina]|Alba County, Romania|[{\"value\":\"ETL\"},...|Traduzione e loca...|    null|[[Dirk Bikkemberg...|Specialties: Inte...|Motorbikes, Histo...|                null|SEO, Product Deve...|{'The Learning Co...|                  []|tafqulhcubhqmfdxd...|[[pub-rajiv-willi...|[[Liones, TU Delf...|\n","+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","only showing top 10 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"svOO4iLPist4","colab_type":"text"},"source":["### Step 1.3: The Extravagent Extraction\n","\n","In our training model, we are interested in when individuals began working at a company.  From creating the schema, you should notice that the collection of companies inviduals worked at are contained in the `experience` field as an array of dictionaries. You should use the `org` for the company name and `start` for the start date. Here is an example of an `experience` field,\n","\n","```\n","{\n","   \"experience\": [\n","     {\n","        \"org\": \"The Walt Disney Company\", \n","        \"title\" : \"Mickey Mouse\",\n","        \"end\" : \"Present\",\n","        \"start\": \"November 1928\",\n","        \"desc\": \"Sailed a boat.\"\n","     },\n","     {\n","        \"org\": \"Walt Disney World Resort\",\n","        \"title\": \"Mickey Mouse Mascot\",\n","        \"start\": \"January 2005\",\n","        \"desc\": \"Took pictures with kids.\"\n","     }\n","   ]\n","}\n","```\n","\n","Your task is to extract each pair of company and start date from these arrays. In Spark, this is known as \"exploding\" a row. An explode will seperate the elements of an array into multiple rows.\n","\n","Create an sdf called `raw_start_dates_sdf` that contains the company and start date for every experience of every individual in `raw_data_sdf`. Drop any row that contains a `null` in either column with `dropna()`. You can sort the elements however you wish (you don't need to if you don't want to). The sdf should look as follows:\n","\n","```\n","+--------------------------+---------------+\n","|org                       |start_date     |\n","+--------------------------+---------------+\n","|Walt Disney World Resort  |January 2005   | \n","|The Walt Disney Company   |November 1928  |\n","|...                       |...            |\n","+--------------------------+---------------+\n","```\n","\n","_Hint_: You may want to do two seperate explodes for `org` and `start`. In an explode, the position of the element in the array can be extracted as well, and used to merge two seperate explodes. Reference the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html).\n","\n","_Note_: Some of the entires in `org` are \"weird\", i.e. made up of non-english letters and characters. Keep them. **DO NOT** edit any name in the original dataframe unless we specify. **DO NOT** drop any row unless there is a `null` value as stated before. This goes for the rest of the homework as well, unless otherwise specified."]},{"cell_type":"code","metadata":{"id":"Kt16tyP0klQX","colab_type":"code","colab":{}},"source":["# TODO: Create [raw_start_dates_sdf]\n","\n","  ##YOUR ANSWER HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-a3xLDEYCcwl","colab_type":"code","outputId":"a29f787c-67df-4d4b-c950-ab719eae4bbf","executionInfo":{"status":"ok","timestamp":1582171479520,"user_tz":300,"elapsed":3607,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["raw_start_dates_sdf.show(4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+--------------------+------------+\n","|                 org|  start_date|\n","+--------------------+------------+\n","|         Iora Health|        2011|\n","|University of Wat...|January 1999|\n","|      Ingersoll Rand|October 2000|\n","|      GE Real Estate|October 2005|\n","+--------------------+------------+\n","only showing top 4 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zSbb0t-d-VFt","colab_type":"text"},"source":["### Step 1.4: The Fortuitous Formatting\n","\n","There are two issues with the values in our `date` column. First, the values are saved as strings, not datetime types. This keeps us from running functions such as `ORDER BY` or `GROUP BY` on common months or years. Second, some values do not have both month and year information or are in other languages. Your task is to filter out and clean the `date` column. We are interested in only those rows that have date in the following format \"(month_name) (year)\", e.g. \"October 2010\".\n","\n","Create an sdf called `filtered_start_dates_sdf` from `raw_start_dates_sdf` with the `date` column filtered in the manner above. Keep only those rows with a start date between January 2000 to December 2011, inclusive. Ensure that any dates that are not in our desired format are ommitted. Drop any row that contains a `null` in either column. The format of the sdf is shown below:\n","```\n","+--------------------------+---------------+\n","|org                       |start_date     |\n","+--------------------------+---------------+\n","|Walt Disney World Resort  |2005-01-01     | \n","|...                       |...            |\n","+--------------------------+---------------+\n","```\n","_Hint_: Refer to the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html) to format the `date` column. In Spark SQL the date format we are interested in is `\"MMM y\"`.\n","\n","_Note_: Spark will return the date in the format above, with the day as `01`. This is ok, since we are interested in the month and year each individual began working and all dates will have `01` as their day."]},{"cell_type":"code","metadata":{"id":"eelgTtOc_MBM","colab_type":"code","colab":{}},"source":["# TODO: Create [filtered_start_dates_sdf]\n","\n","## YOUR ANSWER HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYUuIQHfxueC","colab_type":"code","outputId":"376dd964-1bed-4283-9ab3-d22415c16131","executionInfo":{"status":"ok","timestamp":1582171618456,"user_tz":300,"elapsed":1990,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":454}},"source":["filtered_start_dates_sdf.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+--------------------+----------+\n","|                 org|start_date|\n","+--------------------+----------+\n","|      Ingersoll Rand|2000-10-01|\n","|      GE Real Estate|2005-10-01|\n","|Nissan Motor Co.,...|2002-02-01|\n","|Enthone, a busine...|2007-08-01|\n","|Prem Communicatio...|2003-03-01|\n","|SNAP! Public Rela...|2010-02-01|\n","|Coreobjects India...|2007-04-01|\n","|        AlixPartners|2002-12-01|\n","|       Nilkamal Ltd.|2005-07-01|\n","|   Cognilytics, Inc.|2010-01-01|\n","| Warings Contractors|2004-06-01|\n","|Acuvate Software ...|2010-01-01|\n","|                AtoS|2008-05-01|\n","|             Redleaf|2000-06-01|\n","|ACCENTURE Service...|2007-02-01|\n","|Web Image Consulting|2007-12-01|\n","|                 SAP|2009-01-01|\n","|University of Vic...|2010-10-01|\n","|  Descon Engineering|2008-06-01|\n","|Anglo American Sc...|2008-10-01|\n","+--------------------+----------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LXYYQn2_GYwZ","colab_type":"text"},"source":["### Step 1.5 The Gregarious Grouping\n","\n","We now want to collect the number of individuals that started in the same month and year for each company. Create an sdf called `start_dates_sdf` that has the total number of employees who began working at the same company on the same start date. The format of the sdf is shown below:\n","\n","```\n","+--------------------------+---------------+---------------+\n","|org                       |start_date     |num_employees  |\n","+--------------------------+---------------+---------------+\n","|Walt Disney World Resort  |2005-01-01     |1              |\n","|...                       |...            |...            |\n","+--------------------------+---------------+---------------+\n","```"]},{"cell_type":"code","metadata":{"id":"CxVIyc1CHooV","colab_type":"code","colab":{}},"source":["# TODO: Create [start_dates_sdf]\n","\n","## YOUR ANSWER HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUMwzSPgxxLY","colab_type":"code","outputId":"be715dce-bfef-483b-eea8-1cb4dda12348","executionInfo":{"status":"ok","timestamp":1582171632825,"user_tz":300,"elapsed":4262,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":454}},"source":["start_dates_sdf.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+--------------------+----------+-------------+\n","|                 org|start_date|num_employees|\n","+--------------------+----------+-------------+\n","|               Sharp|2008-02-01|            2|\n","|         Atos Origin|2009-01-01|            3|\n","|            TuneCore|2006-12-01|            2|\n","|            Unilever|2010-05-01|            1|\n","|Pfizer Global Man...|2008-01-01|            1|\n","|   Petronas Carigali|2007-06-01|            1|\n","|ArcelorMittal Dof...|2008-09-01|            3|\n","|     Backcountry.com|2010-11-01|            1|\n","|          Valuebound|2010-04-01|            1|\n","|Corporate Voice W...|2009-04-01|            4|\n","|Technical Wings, ...|2006-08-01|            2|\n","|               Tieto|2010-10-01|            1|\n","| BMO Financial Group|2006-12-01|            1|\n","|Caribbean Moving ...|2009-05-01|            2|\n","|               done!|2011-04-01|            1|\n","|  Det Norske Veritas|2008-08-01|            1|\n","|Lockheed Martin S...|2004-06-01|            3|\n","|                 ING|2003-02-01|            1|\n","|     Barry Callebaut|2006-09-01|            2|\n","| United Bank Limited|2004-01-01|            1|\n","+--------------------+----------+-------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QYScM3FwJnUz","colab_type":"text"},"source":["## Step 2: Hiring Trends Analysis\n","\n","Now we will analyze `start_dates_sdf` to find monthly and annual hiring trends.\n","\n","### Step 2.1: The Marvelous Months\n","\n","Your task is to answer the question: \"On average, what month do most employees start working?\" Create an sdf called `monthly_hires_sdf` which contains the total number of employees that started working on a specific month, at any company and on any year. The `month` column should be of type `int`, i.e. 1-12. The format of the sdf is shown below:\n","\n","```\n","+---------------+---------------+\n","|month          |num_employees  |\n","+---------------+---------------+\n","|1              |...            |\n","|2              |...            |\n","|3              |...            |\n","|...            |...            |\n","+---------------+---------------+\n","```\n","\n","Find the month in which the most employees start working and save its number as an integer to the variable `most_common_month`.\n","\n","_Hint_: Be careful. The start dates we have right now have both month and year. We only want the common months. See if you can find something in the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html) that will help you do this."]},{"cell_type":"code","metadata":{"id":"vLTmvD9WNQH3","colab_type":"code","colab":{}},"source":["# TODO: Create [monthly_hire_sdf] and find the most common month people were\n","# hired. Save its number as an integer to [most_common_month]\n","\n","## YOUR ANSWER HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wfbGoGMRx0w8","colab_type":"code","outputId":"677c245a-237e-4a24-dacf-badc26a23e06","executionInfo":{"status":"ok","timestamp":1582171651407,"user_tz":300,"elapsed":7554,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":302}},"source":["monthly_hires_sdf.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+-----+----+\n","|month| num|\n","+-----+----+\n","|   12|1536|\n","|    1|3717|\n","|    6|3014|\n","|    3|2111|\n","|    5|2757|\n","|    9|2937|\n","|    4|2287|\n","|    8|2652|\n","|    7|2632|\n","|   10|2411|\n","|   11|1886|\n","|    2|1921|\n","+-----+----+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CUtVHRcMUoWp","colab_type":"text"},"source":["### Step 2.2: The Preposterous Percentages\n","\n","The next question we will answer is \"What is the percentage change in hires between 2010 and 2011 for each company?\" Create an sdf called `percentage_change_sdf` that has the percentage change between 2010 and 2011 for each company. The sdf should look as follows:\n","\n","```\n","+---------------------------+--------------------+\n","|org                        |percentage_change   |\n","+---------------------------+--------------------+\n","|Walt Disney World Resort   |12.3                |\n","|...                        |...                 |\n","+---------------------------+--------------------+\n","```\n","\n","_Note_: A percentage change can be positive or negative depending \n","on the difference between the two years.The formula for percent change is given below,\n","\n","$$\\text{% change} = \\frac{P_f-P_i}{P_f} \\times 100$$\n","\n","Here, $P_f$ is the final element (in this case the number of hires in 2011) and $P_i$ is initial element (the number of hires in 2010).\n","\n","_Hint_: This is a **difficult** question. We recommend using a combination of `GROUP BY` and `JOIN`. Keep in mind that operations between columns in SQL dataframes are often easier than those between rows. "]},{"cell_type":"code","metadata":{"id":"_AhhfLXpWq7y","colab_type":"code","colab":{}},"source":["# TODO: Create [percentage_change_sdf]\n","\n","## YOUR ANSWER HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C6yU97TcyCFB","colab_type":"code","outputId":"6f01ed8e-2525-41ad-8463-9e4562b0a941","executionInfo":{"status":"ok","timestamp":1582171722782,"user_tz":300,"elapsed":9896,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":454}},"source":["percentage_change_sdf.where(F.col('percentage_change')>0).show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+--------------------+------------------+\n","|                 org| percentage_change|\n","+--------------------+------------------+\n","|                 OBS|              50.0|\n","|Bank Danamon Indo...| 33.33333333333333|\n","|Bharti Airtel Lim...|              25.0|\n","|       Self Employed|              80.0|\n","|             Avanade| 66.66666666666666|\n","| Columbia University|              25.0|\n","|            Vodafone|              80.0|\n","|      Robert Walters| 66.66666666666666|\n","|  Research In Motion|              75.0|\n","|              Syntel| 66.66666666666666|\n","|           Accenture|              25.0|\n","|              Oracle| 8.333333333333332|\n","|     United Airlines|              50.0|\n","|University of Col...|              50.0|\n","|           Capgemini|              80.0|\n","|              schooX|              50.0|\n","|Cypress Semicondu...| 33.33333333333333|\n","|            Deloitte|58.333333333333336|\n","|         GlobalLogic| 66.66666666666666|\n","|Imperial College ...| 33.33333333333333|\n","+--------------------+------------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QbLTAueQxkHh","colab_type":"code","outputId":"151bb810-3bfb-4378-ccb2-3a5b921212f3","executionInfo":{"status":"ok","timestamp":1582171668919,"user_tz":300,"elapsed":10033,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":454}},"source":["percentage_change_sdf.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+--------------------+------------------+\n","|                 org| percentage_change|\n","+--------------------+------------------+\n","|       GE Healthcare|               0.0|\n","|          321 LAUNCH|               0.0|\n","|Patrick Henry Col...|               0.0|\n","|                 OBS|              50.0|\n","|GEA Process Engin...|            -100.0|\n","|                 UBS|               0.0|\n","|               Nokia|             -50.0|\n","|   Provectus IT, Inc|               0.0|\n","|        BMC Software|               0.0|\n","|Bank Danamon Indo...| 33.33333333333333|\n","|               Hatch|            -100.0|\n","|Swiss-Belhotel In...|               0.0|\n","|              Sulake|               0.0|\n","|            Unilever|-66.66666666666666|\n","|  Schneider Electric|            -100.0|\n","|TCS Express and l...|               0.0|\n","|           Euro RSCG|               0.0|\n","|Loyola University...|               0.0|\n","|National Associat...|               0.0|\n","|           OfficeMax|               0.0|\n","+--------------------+------------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QkF2RfLSXO0u","colab_type":"text"},"source":["## Step 3: Formatting the Training Data\n","\n","\n","Our overaching goal is to train a machine learning (ML) model that will use the monthly hiring trends of a company to predict a positive or negative gain in the company's stock in the first quarter of the following year. A ML model is trained on a set of observations. Each observation contains a set of features, `X`, and a label, `y`. The goal of the ML model is to create a function that takes any `X` as an input and outputs a predicted `y`. \n","\n","The machine learning model we will use is a [Random Forest Classifier](https://builtin.com/data-science/random-forest-algorithm). Each observation we will pass in will have 24 features (columns). These are the number of people hired from Jan to Dec and the company stock price on the last day of each month. The label will be the direction of the company's stock percentage change (positive, `1`, or negative, `-1`) in the first quarter of the following year. Each observation will correspond to a specified company's trends on a specified year. The format of our final training sdf is shown below. The first 26 columns define our observations, `X`, and the last column the label, `y`.\n","```\n","+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n","|org |year |jan_hired |   ...   |dec_hired |jan_stock |   ...   |dec_stock |stock_result |\n","+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n","|IBM |2008 |...       |   ...   |...       |...       |   ...   |...       |1            |\n","|IBM |2009 |...       |   ...   |...       |...       |   ...   |...       |-1           |\n","|... |...  |...       |   ...   |...       |...       |   ...   |...       |...          |\n","+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n","```\n","\n","_Note_: We will use the first three letters of each month in naming, i.e. `jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec`\n","\n","\n","\n","### Step 3.1: The Harmonious Hires\n","\n","Your first task is to create the first half of the training table, i.e. the `jan_hired` through `dec_hired` columns. This will involve reshaping `start_dates_sdf`. Currently, `start_dates_sdf` has columns `org`, `start_date`, and `num_employees`. We want to group the rows together based on common `org` and years and create new columns for the number of employees that started working in each month of that year.\n","\n","Create an sdf called `raw_hirings_for_training_sdf` that has for a single company and a single year, the number of hires in Jan through Dec, and the total number of hires that year. Note that for each company you will have several rows corresponding to years between 2000 and 2011. It is ok if for a given company you don't have a given year. However, ensure that for a given company and given year, each month column has an entry, i.e. if no one was hired the value should be `0`. The format of the sdf is shown below: \n","```\n","+----+-----+----------+---------+----------+----------+\n","|org |year |jan_hired |   ...   |dec_hired |total_num |\n","+----+-----+----------+---------+----------+----------+\n","|IBM |2008 |...       |   ...   |...       |...       |\n","|IBM |2009 |...       |   ...   |...       |...       |\n","|... |...  |...       |   ...   |...       |...       |\n","+----+-----+----------+---------+----------+----------+\n","```\n","_Hint_: This is a **difficult** question. The tricky part is creating the additional columns of monthly hires, specifically when there are missing dates. In our dataset, if a company did not hire anybody in a given date, it will not appear in `start_dates_sdf`. We suggest you look into `CASE` and `WHEN` statements in the [function list](https://spark.apache.org/docs/2.3.0/api/sql/index.html)."]},{"cell_type":"code","metadata":{"id":"btp2wboHqg2J","colab_type":"code","colab":{}},"source":["# TODO: Create [raw_hire_train_sdf]\n","\n","## YOUR SOLUTION HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOY-mC6ryMN0","colab_type":"code","outputId":"e70f0f89-503f-4f39-de89-4f22b0d1e6fc","executionInfo":{"status":"ok","timestamp":1582171748594,"user_tz":300,"elapsed":7619,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":474}},"source":["raw_hire_train_sdf.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+--------------------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n","|                 org|year|jan_hired|feb_hired|mar_hired|apr_hired|may_hired|jun_hired|jul_hired|aug_hired|sep_hired|oct_hired|nov_hired|dec_hired|total_num|\n","+--------------------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n","|   Referencement.com|2006|        0|        0|        0|        0|        1|        0|        0|        0|        0|        0|        0|        0|        1|\n","|NHS Greater Glasg...|2008|        1|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        1|\n","|Bioengineering La...|2010|        0|        0|        0|        0|        0|        0|        1|        0|        0|        0|        0|        0|        1|\n","|             iCODONS|2010|        0|        0|        0|        0|        0|        1|        0|        0|        0|        0|        0|        0|        1|\n","|  Sigmar Recruitment|2008|        2|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        2|\n","|Citibank, Bank Ha...|2010|        3|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        3|\n","|Infosys Technolog...|2006|        0|        0|        0|        0|        0|        0|        0|        0|        5|        0|        0|        0|        5|\n","|              Lohika|2011|        0|        1|        0|        0|        0|        0|        1|        0|        0|        0|        0|        0|        2|\n","|   USA Hoist Company|2001|        0|        0|        0|        0|        0|        0|        0|        0|        4|        0|        0|        0|        4|\n","|  Piramyd retail Ltd|2004|        0|        1|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        1|\n","|SPAR Hyper Market...|2008|        0|        0|        0|        0|        0|        0|        0|        2|        0|        0|        0|        0|        2|\n","|BankIslami Pakist...|2011|        0|        0|        0|        0|        1|        0|        0|        0|        0|        0|        0|        0|        1|\n","|    UHG-OptumInsight|2011|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        4|        0|        4|\n","|    Amplify Mindware|2008|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        1|        1|\n","|           Freshmind|2001|        0|        0|        0|        0|        2|        0|        0|        0|        0|        0|        0|        0|        2|\n","|        PR Animation|2009|        0|        0|        0|        0|        0|        0|        0|        0|        2|        0|        0|        0|        2|\n","|      Active Website|2001|        1|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        1|\n","|Infosys Technolog...|2005|        1|        0|        0|        0|        0|        0|        0|        0|        3|        0|        2|        0|        6|\n","|   Oxfam New Zealand|2011|        0|        0|        0|        3|        0|        0|        0|        0|        0|        0|        0|        0|        3|\n","|          Gruppo Pro|2002|        0|        0|        0|        0|        0|        1|        0|        0|        0|        0|        0|        0|        1|\n","+--------------------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IkXyet6rrczK","colab_type":"text"},"source":["### Step 3.2: The Formidable Filters\n","\n","Create an sdf called `hire_train_sdf` that contains all the observations in `raw_hire_train_sdf` with `total_num` greater than or equal to 10. The format of the sdf is shown below:\n","\n","```\n","+----+-----+----------+---------+----------+----------+\n","|org |year |jan_hired |   ...   |dec_hired |total_num |\n","+----+-----+----------+---------+----------+----------+\n","|IBM |2008 |...       |   ...   |...       |...       |\n","|IBM |2009 |...       |   ...   |...       |...       |\n","|... |...  |...       |   ...   |...       |...       |\n","+----+-----+----------+---------+----------+----------+\n","```\n"]},{"cell_type":"code","metadata":{"id":"dCH4mbNcshq9","colab_type":"code","colab":{}},"source":["# TODO: Create [hire_train_sdf]\n","\n","##YOUR SOLUTION HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OQ-tLj_xXsf","colab_type":"code","outputId":"82d6dc02-c7ef-4ed7-933a-26e799f14f00","executionInfo":{"status":"ok","timestamp":1582171919864,"user_tz":300,"elapsed":6958,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":474}},"source":["hire_train_sdf.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+--------------------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n","|                 org|year|jan_hired|feb_hired|mar_hired|apr_hired|may_hired|jun_hired|jul_hired|aug_hired|sep_hired|oct_hired|nov_hired|dec_hired|total_num|\n","+--------------------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n","|              Oracle|2008|        0|        2|        0|        2|        3|        1|        2|        0|        0|        0|        0|        0|       10|\n","|                 IBM|2009|        0|        0|        2|        0|        0|        1|        3|        0|        3|        1|        0|        0|       10|\n","|           Collabera|2010|       10|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|        0|       10|\n","|           Capgemini|2011|        3|        0|        1|        1|        0|        1|        0|        0|        2|        2|        0|        0|       10|\n","|              Google|2008|        0|        4|        1|        0|        0|        3|        0|        1|        0|        1|        0|        0|       10|\n","|           Microsoft|2010|        0|        2|        2|        4|        0|        0|        0|        0|        0|        1|        2|        0|       11|\n","|           Freelance|2009|        5|        0|        0|        0|        2|        1|        0|        0|        1|        3|        2|        0|       14|\n","|       Cisco Systems|2010|        1|        1|        0|        0|        2|        0|        2|        0|        2|        3|        0|        0|       11|\n","|              Oracle|2010|        0|        0|        0|        0|        0|        1|        7|        2|        0|        0|        1|        0|       11|\n","|           Accenture|2010|        0|        1|        0|        1|        2|        0|        1|        0|        1|        2|        3|        1|       12|\n","|           Microsoft|2009|        0|        1|        0|        0|        0|        4|        2|        1|        1|        1|        0|        0|       10|\n","|              Google|2011|        0|        0|        1|        2|       11|        1|        0|        4|        1|        0|        0|        0|       20|\n","|           Microsoft|2007|        0|        0|        0|        0|        4|        2|        2|        0|        3|        3|        0|        0|       14|\n","|              Oracle|2011|        0|        2|        2|        0|        0|        5|        1|        0|        1|        1|        0|        0|       12|\n","|  Wipro Technologies|2006|        0|        0|        0|        0|        0|        0|        4|        4|        0|        0|        1|        4|       13|\n","|Microsoft Corpora...|2006|        0|        0|        0|        0|        0|        8|        1|        0|        0|        0|        0|        1|       10|\n","|           Accenture|2007|        1|        0|        0|        0|        4|        2|        0|        0|        0|        4|        1|        0|       12|\n","|                 IBM|2011|        3|        0|        0|        0|        4|        0|        4|        3|        0|        0|        0|        0|       14|\n","|           Accenture|2011|        0|        0|        0|        0|        0|        2|        3|        5|        3|        3|        0|        0|       16|\n","|           Microsoft|2011|        0|        0|        0|        3|        3|        0|        2|        3|        0|        0|        1|        3|       15|\n","+--------------------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MN4ik70Hta01","colab_type":"text"},"source":["### Step 3.3: The Stupendous Stocks\n","\n","Now we are ready for the stock data. The stock data we will use is saved in the same S3 bucket as `linkedin.json`. Load the data into the EMR cluster. Run the cell below. ***You do not need to edit this cell***."]},{"cell_type":"code","metadata":{"id":"APv4BxKw643q","colab_type":"code","outputId":"21f24187-66a5-46c3-b0b5-8a47a8e3985f","executionInfo":{"status":"ok","timestamp":1582171929723,"user_tz":300,"elapsed":606,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["# Load stock data\n","raw_stocks_sdf = spark.read.format(\"csv\") \\\n","              .option(\"header\", \"true\") \\\n","              .load(\"./drive/My Drive/Colab Notebooks/stock_prices.csv\")\n","\n","# Creates SQL-accesible table\n","raw_stocks_sdf.createOrReplaceTempView('raw_stocks')\n","\n","# Display the first 10 rows\n","query = '''SELECT *\n","           FROM raw_stocks'''\n","spark.sql(query).show(10)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+------+-----------------+----------+\n","|ticker|    closing_price|      date|\n","+------+-----------------+----------+\n","|   AHH| 8.49315452575684|2013-05-08|\n","|   AHH| 8.47115135192871|2013-05-09|\n","|   AHH| 8.50782203674316|2013-05-10|\n","|   AHH| 8.54449367523193|2013-05-13|\n","|   AHH|8.456483840942381|2013-05-14|\n","|   AHH| 8.50782203674316|2013-05-15|\n","|   AHH| 8.61050128936768|2013-05-16|\n","|   AHH|8.625171661376951|2013-05-17|\n","|   AHH| 8.60316944122314|2013-05-20|\n","|   AHH|8.676511764526369|2013-05-21|\n","+------+-----------------+----------+\n","only showing top 10 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JUCdr3zDUAFH","colab_type":"text"},"source":["Run the cell below to see the types of the columns in our data frame. These are not correct. We could have defined a schema when reading in data but we will handle this issue in another manner. You will do this in Step 3.4.2."]},{"cell_type":"code","metadata":{"id":"oNTGEfxsisqs","colab_type":"code","outputId":"7e456ca9-4ad9-48ac-9335-4393ab78c295","executionInfo":{"status":"ok","timestamp":1582171931898,"user_tz":300,"elapsed":329,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Print types of SDF\n","raw_stocks_sdf.dtypes"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('ticker', 'string'), ('closing_price', 'string'), ('date', 'string')]"]},"metadata":{"tags":[]},"execution_count":119}]},{"cell_type":"markdown","metadata":{"id":"0DdnoFkP7mxz","colab_type":"text"},"source":["### Step 3.4 The Clairvoyant Cleaning\n","\n","We now want to format the stock data set into the second half of the training table. We will then merge it with `hire_train` based off the common `org` and `year` fields.\n","\n","#### Step 3.4.1 The Ubiquitous UDF\n","\n","The companies in our stock dataset are defined by their stock tickers. Thus, we would not be able to merge it with the `org` field in `hire_train_sdf`. We must convert them to that format. Often times when using Spark, there may not be a built-in SQL function that can do the operation we desired. Instead, we can create one on our own with a user-defined function (udf).\n","\n","A udf is defined as a normal Python function and then registered to be used as a Spark SQL function. Your task is to create a udf, `TICKER_TO_NAME()` that will convert the ticker field in `raw_stocks` to the company's name. This will be done using the provided `ticker_to_name_dict` dictionary. We are only interested in the companies in that dictionary.\n","\n","Fill out the function `ticker_to_name()` below. Then use `spark.udf.register()` to register it as a SQL function. The command is provided. ***You do not need to edit it***. Note, we have defined the udf as returning `StringType()`. Ensure that your function returns this. You must also deal with any potential `null` cases."]},{"cell_type":"code","metadata":{"id":"P4cJWZsr8iNC","colab_type":"code","outputId":"b2c28a48-9806-49d1-93e1-6a3be981291a","executionInfo":{"status":"ok","timestamp":1582171948113,"user_tz":300,"elapsed":358,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# TODO: Fill out [ticker_to_name()] and register it as a udf.\n","\n","## YOUR SOLUTION HERE\n","\n","def ticker_to_name(ticker):\n","  \n","# Register udf as a SQL function. DO NOT EDIT\n","spark.udf.register(\"TICKER_TO_NAME\", ticker_to_name, StringType())\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.ticker_to_name>"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"markdown","metadata":{"id":"u9YOYO9L-_GS","colab_type":"text"},"source":["#### Step 3.4.2: The Fastidious Filters\n","\n","With our new `TICKER_TO_NAME()` function we will begin to wrangle `raw_stocks_sdf`.\n","\n","Create an sdf called `filter_1_stocks_sdf` as follows. Convert all the ticker names in `raw_stocks_sdf` to the company names and save it as `org`. Next, convert the `date` field to a datetime type. As explained before this will help order and group the rows in future steps. Then, convert the type of the values in `closing_price` to `float`. This will take care of the `dtypes` issue we saw in Step 3.3.\n","\n","Drop any company names that do not appear in `ticker_to_name_dict`. Keep any date between January 1st 2001 and December 4th 2012 inclusive, in the format shown below (note this is a datetime object not a string):\n","\n","```\n","+----+------------+--------------+\n","|org |date        |closing_price |\n","+----+------------+--------------+\n","|IBM |2000-01-03  |...           |\n","|... |...         |...           |\n","+----+------------+--------------+\n","```\n","_Hint_: You will use a similar function to filter the dates as in Step 1.4. In Spark SQL the format for the `date` field in `raw_stocks_sdf` is `\"yyyy-MM-dd\"`."]},{"cell_type":"code","metadata":{"id":"RuiitnWlBYJ7","colab_type":"code","colab":{}},"source":["# TODO: Create [filter_1_stocks_sdf]\n","\n","## YOUR SOLUTION HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1y-8pHAlzCyt","colab_type":"code","outputId":"d0bb4596-6691-4dc4-facb-eb35caa46ea0","executionInfo":{"status":"ok","timestamp":1582171988536,"user_tz":300,"elapsed":26661,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":454}},"source":["filter_1_stocks_sdf.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+------+----------+-------------+\n","|   org|      date|closing_price|\n","+------+----------+-------------+\n","|Pfizer|2001-01-02|     24.92212|\n","|Pfizer|2001-01-03|    23.537558|\n","|Pfizer|2001-01-04|       22.592|\n","|Pfizer|2001-01-05|    22.895922|\n","|Pfizer|2001-01-08|    22.625769|\n","|Pfizer|2001-01-09|    23.368702|\n","|Pfizer|2001-01-10|    22.862156|\n","|Pfizer|2001-01-11|    22.152983|\n","|Pfizer|2001-01-12|    22.389381|\n","|Pfizer|2001-01-16|     22.52445|\n","|Pfizer|2001-01-17|     22.01791|\n","|Pfizer|2001-01-18|    22.220531|\n","|Pfizer|2001-01-19|    22.355614|\n","|Pfizer|2001-01-22|    22.760843|\n","|Pfizer|2001-01-23|    23.166077|\n","|Pfizer|2001-01-24|    23.199846|\n","|Pfizer|2001-01-25|    24.010323|\n","|Pfizer|2001-01-26|    23.942793|\n","|Pfizer|2001-01-29|    23.395712|\n","|Pfizer|2001-01-30|     23.92523|\n","+------+----------+-------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ne5NaT-6CLns","colab_type":"text"},"source":["#### Step 3.4.3: The Magnanimous Months\n","\n","The data in `filter_1_stocks_sdf` gives closing prices on a daily basis. Since we are interested in monthly trends, we will only keep the closing price on the **last trading day of each month**.\n","\n","Create an sdf `filter_2_stocks_sdf` that contains only the closing prices for the last trading day of each month. Note that a trading day is not simply the last day of each month, as this could be on a weekend when the market is closed . The format of the sdf is shown below:\n","\n","```\n","+----+------------+--------------+\n","|org |date        |closing_price |\n","+----+------------+--------------+\n","|IBM |2000-01-31  |...           |\n","|... |...         |...           |\n","+----+------------+--------------+\n","```\n","\n","  _Hint_: It may be helpful to create an intermediate dataframe that will help you filter out the specific dates you desire."]},{"cell_type":"code","metadata":{"id":"AIx5LUuDD4q_","colab_type":"code","colab":{}},"source":["# TODO: Create [filter_2_stocks_sdf]\n","\n","## YOUR SOLUTION HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g94lqwFMzOMI","colab_type":"code","outputId":"7082b4e8-5c46-4898-ea63-33b1f817ba84","executionInfo":{"status":"ok","timestamp":1582172134435,"user_tz":300,"elapsed":124884,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":454}},"source":["filter_2_stocks_sdf.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+-----------------+----------+-------------+\n","|              org|      date|closing_price|\n","+-----------------+----------+-------------+\n","|        Accenture|2004-05-28|    18.978838|\n","|             Citi|2004-06-30|    435.70758|\n","|             Citi|2004-11-30|     420.0729|\n","|             HSBC|2012-12-04|    37.099277|\n","|              IBM|2001-10-31|     76.18288|\n","|              IBM|2004-08-31|     61.02406|\n","|Johnson & Johnson|2001-12-31|     38.02878|\n","|Johnson & Johnson|2006-09-29|    45.635067|\n","|      Kraft Foods|2003-09-30|    13.226012|\n","|        Microsoft|2012-10-31|    24.469625|\n","|            Nokia|2001-01-31|    20.169237|\n","|         Novartis|2008-04-30|     34.72512|\n","|           Oracle|2001-04-30|    14.432037|\n","| Procter & Gamble|2010-09-30|    46.444077|\n","|         Unilever|2002-11-29|    8.7186165|\n","|         Unilever|2007-08-31|    20.686466|\n","|               BP|2003-08-29|    20.518118|\n","|  Bank of America|2003-11-28|     26.03784|\n","|  Bank of America|2008-07-31|    29.145456|\n","| Barclays Capital|2004-10-29|     23.31978|\n","+-----------------+----------+-------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AG4bACKKEQNl","colab_type":"text"},"source":["#### Step 3.4.4: The Rambunctious Reshape\n","\n","Now, we will begin to shape our dataframe into the format of the final training sdf.\n","\n","Create an sdf `filter_3_stocks_sdf` that has for a single company and a single year, the closing stock price for the last trading day of each month in that year. This is similar to the table you created in Step 3.1. In this case since we cannot make a proxy for the closing price if the data is not avaliable, drop any rows containing any `null` values, in any column. The format of the sdf is shown below:\n","\n","```\n","+----+-----+----------+---------+----------+\n","|org |year |jan_stock |   ...   |dec_stock |\n","+----+-----+----------+---------+----------+\n","|IBM |2008 |...       |   ...   |...       |\n","|IBM |2009 |...       |   ...   |...       |\n","|... |...  |...       |   ...   |...       |\n","+----+-----+----------+---------+----------+\n","```\n"]},{"cell_type":"code","metadata":{"id":"AucLEgvwIr_0","colab_type":"code","colab":{}},"source":["# TODO: Create [filter_3_stocks_sdf]\n","\n","## YOUR SOLUTION HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_yWdBbxOzR0L","colab_type":"code","outputId":"5bcc873b-3987-4f74-da4b-fdf9b8c33f2a","executionInfo":{"status":"ok","timestamp":1582172260546,"user_tz":300,"elapsed":125794,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":474}},"source":["filter_3_stocks_sdf.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+---------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n","|      org|year|jan_stock|feb_stock|mar_stock|apr_stock|may_stock|jun_stock|jul_stock|aug_stock|sep_stock|oct_stock|nov_stock|dec_stock|\n","+---------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n","|Accenture|2002|19.881117|20.197304|20.590609|16.534184| 16.07918|14.652494|12.724533|12.685975|11.012507|13.017582|14.845289|13.873597|\n","|Accenture|2003|12.763092|11.829961| 11.95335|12.354365| 13.51114|13.950715|14.976391|16.318249|17.228245|18.045704|19.202477|20.297556|\n","|Accenture|2004|18.253922|17.814348|19.125355|18.331043|18.978838|21.192133|18.994257|  20.1279|20.860518|18.670366| 20.00451|20.821964|\n","|Accenture|2005| 20.08934|19.703743|18.624092|16.734688| 17.95316| 17.48274|19.310446|18.816887|19.634342|20.535488|22.197994|22.533619|\n","|Accenture|2006|  24.6098|25.491789|23.470243|22.689726|21.971642|22.104328|22.838018|23.150227|24.750296| 25.97206|26.595509| 29.14458|\n","|Accenture|2007|29.791712|28.237013|30.415163|30.857115| 32.30921| 33.84813| 33.24834| 32.52229|31.764671|31.142376|  27.5616|28.733923|\n","|Accenture|2008|27.609447|28.111877|28.048069| 29.94612| 32.55394|  32.4742|33.303593|32.984596|30.304995|26.776814|25.099716| 26.56616|\n","|Accenture|2009|25.569622|23.649471|22.272146|23.843916|24.249016|27.108992|28.413393|  26.7363|30.195814| 30.62453| 33.89512|34.275032|\n","|Accenture|2010|33.853825|  33.0114|34.646694|36.358814|31.259914| 32.20138|  33.0262|30.493412|35.400692| 37.62049|36.450886|40.801105|\n","|Accenture|2011|43.308575| 43.31699|46.253593| 48.46163| 48.68219|51.252438|50.166653|45.458767|44.686832|51.729073|49.728935| 45.69431|\n","|Accenture|2012|49.222466|51.111015| 55.36883|56.357517|49.546024| 52.14048|52.322685| 53.45071| 60.76548|  59.1729| 59.62059| 60.22627|\n","|       BP|2001|23.284948|22.569365|22.578472|24.607897|24.439312|22.818876|22.622051| 23.43871|22.650976|22.268621|20.483982|21.569159|\n","|       BP|2002| 21.66655|23.143597|24.801704|23.727448|24.011484|23.738796|21.815804|22.175005|18.905603|18.218563|18.753345|19.442062|\n","|       BP|2003|18.657688|18.401495|18.633265|18.609118|20.413568|20.476917|20.247877|20.518118|20.705015|20.842722|21.189156|24.494848|\n","|       BP|2004|23.626242| 24.62511|25.626127|26.476992|26.732676|27.020176|28.427435|27.295452|29.242222| 29.60819|31.408638|29.898357|\n","|       BP|2005|30.522942|33.509933|32.209183|31.434927| 31.33362|32.468285|34.289993|35.864902|  37.1604|  34.8264| 34.81307|33.956482|\n","|       BP|2006| 38.23409| 35.40757|36.750927|39.299095|37.969193| 37.38381| 38.94661| 36.85579|35.518032|36.341263|  37.1939| 36.65849|\n","|       BP|2007|34.697178|33.958885|35.730206|37.148373| 37.31543| 40.17214| 38.64633|37.871956|38.990818|43.848495|41.232788|41.476532|\n","|       BP|2008| 36.25017| 37.22258|34.801125| 41.76709|42.077793|40.371696|35.653835|33.910072|29.520536|29.243975|29.170252| 28.00201|\n","|       BP|2009|25.443832|23.450315|24.514015|25.956736|30.800116|29.667667| 31.13612|32.551403|33.677563|35.822357| 36.69092|37.197845|\n","+---------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"82OQKp-nIulq","colab_type":"text"},"source":["#### Step 3.4.5: The Decisive Direction\n","\n","The final element in our training set is the binary output for each case, i.e. the `y` label. \n","\n","Create an sdf `stocks_train_sdf` from `filter_3_stocks_sdf` with an additional column `direction`. This should be the direction of percentage change in the closing stock price, i.e. `1` for positive or `-1` for negative, in the first quarter of a given year. The quarter of a year begins in January and ends in April, inclusive. We want to know the percent change between these two months. Reference Step 2.2 for the percent change formula. The format of the sdf is shown below:\n","\n","```\n","+----+-----+----------+---------+----------+-------------+\n","|org |year |jan_stock |   ...   |dec_stock |direction    |\n","+----+-----+----------+---------+----------+-------------+\n","|IBM |2008 |...       |   ...   |...       |1.0          |\n","|IBM |2009 |...       |   ...   |...       |-1.0         |\n","|... |...  |...       |   ...   |...       |...          |\n","+----+-----+----------+---------+----------+-------------+\n","```"]},{"cell_type":"code","metadata":{"id":"yEFJIfyZKf7B","colab_type":"code","colab":{}},"source":["# TODO: Create [stocks_train_sdf]\n","\n"," ## YOUR SOLUTION HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DMH6HO7PziEf","colab_type":"code","outputId":"407253c4-1860-4d4a-b035-e79bb1e1837c","executionInfo":{"status":"ok","timestamp":1582172386778,"user_tz":300,"elapsed":126202,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":474}},"source":["stocks_train_sdf.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+---------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n","|      org|year|jan_stock|feb_stock|mar_stock|apr_stock|may_stock|jun_stock|jul_stock|aug_stock|sep_stock|oct_stock|nov_stock|dec_stock|direction|\n","+---------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n","|Accenture|2002|19.881117|20.197304|20.590609|16.534184| 16.07918|14.652494|12.724533|12.685975|11.012507|13.017582|14.845289|13.873597|     -1.0|\n","|Accenture|2003|12.763092|11.829961| 11.95335|12.354365| 13.51114|13.950715|14.976391|16.318249|17.228245|18.045704|19.202477|20.297556|     -1.0|\n","|Accenture|2004|18.253922|17.814348|19.125355|18.331043|18.978838|21.192133|18.994257|  20.1279|20.860518|18.670366| 20.00451|20.821964|      1.0|\n","|Accenture|2005| 20.08934|19.703743|18.624092|16.734688| 17.95316| 17.48274|19.310446|18.816887|19.634342|20.535488|22.197994|22.533619|     -1.0|\n","|Accenture|2006|  24.6098|25.491789|23.470243|22.689726|21.971642|22.104328|22.838018|23.150227|24.750296| 25.97206|26.595509| 29.14458|     -1.0|\n","|Accenture|2007|29.791712|28.237013|30.415163|30.857115| 32.30921| 33.84813| 33.24834| 32.52229|31.764671|31.142376|  27.5616|28.733923|      1.0|\n","|Accenture|2008|27.609447|28.111877|28.048069| 29.94612| 32.55394|  32.4742|33.303593|32.984596|30.304995|26.776814|25.099716| 26.56616|      1.0|\n","|Accenture|2009|25.569622|23.649471|22.272146|23.843916|24.249016|27.108992|28.413393|  26.7363|30.195814| 30.62453| 33.89512|34.275032|     -1.0|\n","|Accenture|2010|33.853825|  33.0114|34.646694|36.358814|31.259914| 32.20138|  33.0262|30.493412|35.400692| 37.62049|36.450886|40.801105|      1.0|\n","|Accenture|2011|43.308575| 43.31699|46.253593| 48.46163| 48.68219|51.252438|50.166653|45.458767|44.686832|51.729073|49.728935| 45.69431|      1.0|\n","|Accenture|2012|49.222466|51.111015| 55.36883|56.357517|49.546024| 52.14048|52.322685| 53.45071| 60.76548|  59.1729| 59.62059| 60.22627|      1.0|\n","|       BP|2001|23.284948|22.569365|22.578472|24.607897|24.439312|22.818876|22.622051| 23.43871|22.650976|22.268621|20.483982|21.569159|      1.0|\n","|       BP|2002| 21.66655|23.143597|24.801704|23.727448|24.011484|23.738796|21.815804|22.175005|18.905603|18.218563|18.753345|19.442062|      1.0|\n","|       BP|2003|18.657688|18.401495|18.633265|18.609118|20.413568|20.476917|20.247877|20.518118|20.705015|20.842722|21.189156|24.494848|     -1.0|\n","|       BP|2004|23.626242| 24.62511|25.626127|26.476992|26.732676|27.020176|28.427435|27.295452|29.242222| 29.60819|31.408638|29.898357|      1.0|\n","|       BP|2005|30.522942|33.509933|32.209183|31.434927| 31.33362|32.468285|34.289993|35.864902|  37.1604|  34.8264| 34.81307|33.956482|      1.0|\n","|       BP|2006| 38.23409| 35.40757|36.750927|39.299095|37.969193| 37.38381| 38.94661| 36.85579|35.518032|36.341263|  37.1939| 36.65849|      1.0|\n","|       BP|2007|34.697178|33.958885|35.730206|37.148373| 37.31543| 40.17214| 38.64633|37.871956|38.990818|43.848495|41.232788|41.476532|      1.0|\n","|       BP|2008| 36.25017| 37.22258|34.801125| 41.76709|42.077793|40.371696|35.653835|33.910072|29.520536|29.243975|29.170252| 28.00201|      1.0|\n","|       BP|2009|25.443832|23.450315|24.514015|25.956736|30.800116|29.667667| 31.13612|32.551403|33.677563|35.822357| 36.69092|37.197845|      1.0|\n","+---------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Fd2nviNpM2dF","colab_type":"text"},"source":["### Step 3.5: The Capricious Combination\n","\n","Now that we have individually created the two halfs of our training data we will merge them together to create the final training sdf we showed in the beginning of Step 3.\n","\n","Create an sdf called `training_sdf` in the format of the one shown at the beginning of Step 3. Note that in our definition for the `stock_result` column, the `stock_result` value for a particular year corresponds to the direction of the stock percentage change in the **following** year. For example, the stock_result in the `2008` row for `IBM` will contain the direction of IBM's stock in the first quarter of 2009. The format of the sdf is shown below:\n","```\n","+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n","|org |year |jan_hired |   ...   |dec_hired |jan_stock |   ...   |dec_stock |stock_result |\n","+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n","|IBM |2008 |...       |   ...   |...       |...       |   ...   |...       |-1.0         |\n","|IBM |2009 |...       |   ...   |...       |...       |   ...   |...       |1.0          |\n","|... |...  |...       |   ...   |...       |...       |   ...   |...       |...          |\n","+----+-----+----------+---------+----------+----------+---------+----------+-------------+\n","```"]},{"cell_type":"code","metadata":{"id":"8ZIb6QkcO5RB","colab_type":"code","colab":{}},"source":["# TODO: Create [training_sdf]\n","\n","## YOUR SOLUTION HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XtA1vdPEz3gn","colab_type":"code","outputId":"7cb607d3-ccf8-4c20-8164-93a13dff035b","executionInfo":{"status":"ok","timestamp":1582172660612,"user_tz":300,"elapsed":248033,"user":{"displayName":"Leshang Chen","photoUrl":"","userId":"00573172153387237137"}},"colab":{"base_uri":"https://localhost:8080/","height":474}},"source":["training_sdf.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+------------------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+------------+\n","|               org|year|jan_hired|feb_hired|mar_hired|apr_hired|may_hired|jun_hired|jul_hired|aug_hired|sep_hired|oct_hired|nov_hired|dec_hired|jan_stock|feb_stock|mar_stock|apr_stock|may_stock|jun_stock|jul_stock|aug_stock|sep_stock|oct_stock|nov_stock|dec_stock|stock_result|\n","+------------------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+------------+\n","|Wipro Technologies|2006|        0|        0|        0|        0|        0|        0|        4|        4|        0|        0|        1|        4|3.4224024| 3.201156|3.4270115|3.2933416|2.7632725|2.9850497|2.9012916|2.9501507|3.0827675|3.3875542|3.6481357|3.7574866|        -1.0|\n","|               IBM|2011|        3|        0|        0|        0|        4|        0|        4|        3|        0|        0|        0|        0|128.32043|128.73308| 129.6794|135.65163|134.94023|137.03311|145.26068|137.91862|140.29338|148.12355|151.43352|148.11487|         1.0|\n","|         Microsoft|2009|        0|        1|        0|        0|        0|        4|        2|        1|        1|        1|        0|        0|13.403088|12.745262|14.497245|15.988795|16.590673|18.877947|  18.6794|19.686914|20.541473|22.146774| 23.59235| 24.45069|         1.0|\n","|            Google|2011|        0|        0|        1|        2|       11|        1|        0|        4|        1|        0|        0|        0|300.48047|307.00702|293.67368|272.32233|264.77478|253.44345|302.14716|270.75076|257.77777| 296.6166|  299.995| 323.2733|         1.0|\n","|            Google|2010|        0|        0|        4|        0|        1|        4|        1|        0|        1|        0|        5|        0|265.23523|263.66367|283.84384| 263.1131|243.05806| 222.6977|242.66766|225.23523|263.15817|307.15717|278.13315| 297.2823|        -1.0|\n","|         Microsoft|2008|        0|        0|        1|        0|        0|        7|        2|        0|        1|        0|        0|        4|25.089111|21.014587|21.926254|22.034414|21.960442|21.332335|19.944298|21.246143|20.779022|17.384617| 15.84856|15.237188|         1.0|\n","|         Accenture|2004|        0|        1|        3|        0|        0|        2|        1|        5|        0|        0|        1|        0|18.253922|17.814348|19.125355|18.331043|18.978838|21.192133|18.994257|  20.1279|20.860518|18.670366| 20.00451|20.821964|        -1.0|\n","|         Microsoft|2006|        0|        0|        1|        0|        4|        0|        1|        1|        0|        5|        1|        4|21.069159|20.179277|20.434622|18.136572|  17.0764|17.566456| 18.13944|19.447233|20.695787|21.724894| 22.29271|22.672358|        -1.0|\n","|            Oracle|2010|        0|        0|        0|        0|        0|        1|        7|        2|        0|        0|        1|        0|20.790052|22.223534|23.179193|23.368156|20.387291|19.384636|  21.3996|19.770187|24.305382| 26.64452|24.531458|28.385752|         1.0|\n","|         Accenture|2010|        0|        1|        0|        1|        2|        0|        1|        0|        1|        2|        3|        1|33.853825|  33.0114|34.646694|36.358814|31.259914| 32.20138|  33.0262|30.493412|35.400692| 37.62049|36.450886|40.801105|         1.0|\n","|         Microsoft|2011|        0|        0|        0|        3|        3|        0|        2|        3|        0|        0|        1|        3|22.707973|21.894892|20.914648|21.351227| 20.73666| 21.55751|22.718294|22.194193|20.767426|22.219223| 21.50385|21.823296|         1.0|\n","|         Microsoft|2010|        0|        2|        2|        4|        0|        0|        0|        0|        0|        1|        2|        0| 22.60566|23.106277|23.605959|24.613388|20.887058|18.628342|20.895153|19.102102| 19.93228|21.706566|20.685299|22.855371|        -1.0|\n","|               IBM|2004|        0|        0|        2|        0|        2|        0|        3|        0|        0|        4|        1|        0| 71.09023|  69.2465| 65.90257|63.269043|63.699318|63.382954|  62.6064| 61.02406|61.780605|64.670044|68.036644|71.169914|        -1.0|\n","|         Microsoft|2007|        0|        0|        0|        0|        4|        2|        2|        0|        3|        3|        0|        0|23.431643|21.463324|21.234745| 22.81192| 23.45911|22.526556|22.159647|22.037878|22.597847|  28.2358| 25.85872|27.397928|        -1.0|\n","|     Cisco Systems|2010|        1|        1|        0|        0|        2|        0|        2|        0|        2|        3|        0|        0|18.139093|19.640594|21.012932|21.739466|18.696098|17.202671|18.623446|16.137093|17.678951|18.453924|15.467071|16.330835|        -1.0|\n","|         Accenture|2011|        0|        0|        0|        0|        0|        2|        3|        5|        3|        3|        0|        0|43.308575| 43.31699|46.253593| 48.46163| 48.68219|51.252438|50.166653|45.458767|44.686832|51.729073|49.728935| 45.69431|         1.0|\n","|            Oracle|2011|        0|        2|        2|        0|        0|        5|        1|        0|        1|        1|        0|        0|29.094435|29.884706|30.366133|32.722794| 31.13943|29.947361|27.876394|25.588306|26.199068|29.932493| 28.63545|23.429007|         1.0|\n","|            Google|2008|        0|        4|        1|        0|        0|        3|        0|        1|        0|        1|        0|        0|282.43243|235.82582|220.45546|287.43243| 293.1932|263.47348| 237.1121|231.87688|200.46046|179.85986|146.62663|153.97897|         1.0|\n","|               IBM|2006|        6|        1|        0|        0|        0|        2|        0|        2|        2|        2|        2|        0|  59.2463| 58.62066| 60.24984|60.154823|58.583298| 56.32501|56.757626|59.604603|60.318645| 67.96708| 67.88484| 71.74734|         1.0|\n","|            Oracle|2008|        0|        2|        0|        2|        3|        1|        2|        0|        0|        0|        0|        0|18.352621|16.789745|17.468481|18.620543| 20.39775|18.754501|19.227835|19.585062| 18.13828|16.334276|14.369522|15.834158|         1.0|\n","+------------------+----+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+---------+------------+\n","only showing top 20 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FP6uIJUfz4XL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}