{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework3_no_Solution_New_Data.ipynb","provenance":[{"file_id":"1K0hp-Y5R7FHa3AwfAj2tCw3ueXOlJhay","timestamp":1573223441486}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"syxh_fwyTAVU","nbgrader":{"cell_type":"markdown","checksum":"07f80342af7b3f53232fa665711a4f8d","grade":false,"grade_id":"cell-fe1ccb2b0cf04ec5","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Homework: Understanding Performance using a LinkedIn Dataset\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"-kIU5Hyh_ze0","nbgrader":{"cell_type":"markdown","checksum":"66956d89cf4b869a752255872c9acf23","grade":false,"grade_id":"cell-388a39fc469b703f","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["This homework focuses on understanding performance using a LinkedIn dataset.  It is the same dataset that was used in the module entitled \"Modeling Data and Knowledge\"."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"84c0VetBS-Ae","colab":{}},"source":["!pip install pymongo[tls,srv]\n","!pip install swifter\n","!pip install lxml"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gvINUcJg9mPs","colab_type":"code","colab":{}},"source":["\n","import pandas as pd\n","import numpy as np\n","import json\n","import sqlite3\n","from lxml import etree\n","import urllib\n","import zipfile\n","\n","import time\n","import swifter\n","from pymongo import MongoClient\n","from pymongo.errors import DuplicateKeyError, OperationFailure\n","from sklearn.utils import shuffle"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"62VpoB2eU6lz","nbgrader":{"cell_type":"markdown","checksum":"2c1ade1ed098ba7b6d850eebda8d566f","grade":false,"grade_id":"cell-2bb8ffaa9ddfdf13","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Step 1: Acquire and load the data\n","\n","We will pull a zipfile with the LinkedIn dataset from an url / Google Drive so that it can be efficiently parsed locally. The detailed steps are covered by  \"Modeling Data and Knowledge\" Module, and you should refer to the instructor notes of that module if you haven't done so. \n","\n","The cell below will download/open the file, and may take a while. \n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y5pOCyFCgTtY","colab":{}},"source":["url = 'https://raw.githubusercontent.com/chenleshang/OpenDS4All/master/Module3/homework3filewrapper.py'\n","urllib.request.urlretrieve(url,filename='homework3filewrapper.py')\n","# url = 'https://upenn-bigdataanalytics.s3.amazonaws.com/linkedin.zip'\n","# filehandle, _ = urllib.request.urlretrieve(url,filename='local.zip')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"641d22e91a3dae8a1ae26cfd88714eea","grade":false,"grade_id":"cell-f81f52c4a3ec94ad","locked":true,"schema_version":3,"solution":false,"task":false},"id":"VY64R_HyosKJ","colab_type":"text"},"source":["The next cell creates a pointer to the (abbreviated)  LinkedIn dataset, and imports a script that will be used to prepare the dataset to manipulate in this homework. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JwpR-SU_gzMC","colab":{}},"source":["def fetch_file(fname):\n","    zip_file_object = zipfile.ZipFile(filehandle, 'r')\n","    for file in zip_file_object.namelist():\n","        file = zip_file_object.open(file)\n","        if file.name == fname: return file\n","    return None\n","\n","# linked_in = fetch_file('test_data_10000.json')\n","\n","from homework3filewrapper import *"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"ud1c3IAbhszs","nbgrader":{"cell_type":"markdown","checksum":"468120adc95b35087ae865792ad51f00","grade":false,"grade_id":"cell-63e729eee9ebf28b","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["The next cell replays the data preparation for the LinkedIn dataset done in the module \"Modeling Data and Knowledge\". After this, you should have eleven dataframes with the following names. The first nine are as in the lecture notebook; the last two are constructed using queries over the first nine, and their meanings are given  below. \n","\n","1. `people_df`\n","2. `names_df`: Stores the first and last name of each person indexed by ID. \n","3. `education_df`\n","4. `groups_df`\n","5. `skills_df`\n","6. `experience_df`\n","7. `honors_df`\n","8. `also_view_df`\n","9. `events_df`\n","10. `recs_df`: 20 pairs of people with the most shared/common skills in descending order. We will use this to make a recommendation for a potential employer and position to each person.\n","11. `last_job_df`: Person name, and the title and org corresponding to the person's last (most recent) employment experience (a three column dataframe).\n","\n","The number of rows that are extracted from the dataset can be changed using LIMIT.  Here, we are limiting it to 10,000; you can set it to something much smaller (e.g. 1,000) while debugging your code. \n","\n","The data is also being stored in an SQLite database so that you can see the effect of indexing on the performance of queries.\n","\n"]},{"cell_type":"code","metadata":{"id":"v0zOPjRuR2K8","colab_type":"code","colab":{}},"source":["# If use a file on Google Drive, then mount it to Colab. \n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BabSIiN9JNP","colab_type":"code","colab":{}},"source":["# use open() to open a local file, or to use fetch_file() to get that file from a remote zip file. \n","people_df, names_df, education_df, groups_df, skills_df, experience_df, honors_df, also_view_df, events_df, recs_df, last_job_df =\\\n","    data_loading(file=open('/content/drive/My Drive/Colab Notebooks/test_data_10000.json'), dbname='linkedin.db', filetype='localobj', LIMIT=10000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVRbVvz2uvTQ","colab_type":"code","colab":{}},"source":["conn = sqlite3.connect('linkedin.db')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"nWX716u93MLx","nbgrader":{"cell_type":"code","checksum":"21c423cb12953ae8180755b8dedfb787","grade":true,"grade_id":"0-2-1-san","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity Check 1.1 - please do not modify or delete this cell!\n","\n","recs_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"fD1WW-4qiHP8","nbgrader":{"cell_type":"code","checksum":"d98de5be163a08b9ae53b904ca7e514b","grade":true,"grade_id":"0-2-2-san","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity Check 1.2 - please do not modify or delete this cell!\n","\n","names_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"N7AO3Um0iHYE","nbgrader":{"cell_type":"code","checksum":"1e77e70a7887a6508367b725680c5db0","grade":true,"grade_id":"0-2-3-san","locked":true,"points":1,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity Check 1.3 - please do not modify or delete this cell!\n","\n","last_job_df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"ded9460c8b1a046b90326da3f3dc2b26","grade":false,"grade_id":"cell-3002547b83f8e405","locked":true,"schema_version":3,"solution":false,"task":false},"id":"-nIXPaUN85Qc"},"source":["# Step 2: Compare Evaluation Orders using DataFrames\n","\n","We will now explore the effect of various optimizations, including reordering execution steps and (in the case of database operations) creating indices."]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"markdown","checksum":"3cdd63a24299f9cb836524122a955f6b","grade":false,"grade_id":"cell-3efe42cac0356604","locked":true,"schema_version":3,"solution":false,"task":false},"id":"1AHO6W3rosNO","colab_type":"text"},"source":["We'll start with the code from our lecture notebooks, which does joins between dataframes.  The next cell creates two functions, merge and merge_map, which we explore in terms of efficiency.  **You do not need to modify this cell.**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3NnnYP9pKMp1","colab":{}},"source":["# Join using nested loops\n","def merge(S,T,l_on,r_on):\n","    ret = pd.DataFrame()\n","    count = 0\n","    S_ = S.reset_index().drop(columns=['index'])\n","    T_ = T.reset_index().drop(columns=['index'])\n","    for s_index in range(0, len(S)):\n","        for t_index in range(0, len(T)):\n","            count = count + 1\n","            if S_.loc[s_index, l_on] == T_.loc[t_index, r_on]:\n","                ret = ret.append(S_.loc[s_index].append(T_.loc[t_index].drop(labels=r_on)), ignore_index=True)\n","\n","    print('Merge compared %d tuples'%count)\n","    return ret\n","  \n","# Join using a *map*, which is a kind of in-memory index\n","# from keys to (single) values\n","def merge_map(S,T,l_on,r_on):\n","    ret = pd.DataFrame()\n","    T_map = {}\n","    count = 0\n","    # Take each value in the r_on field, and\n","    # make a map entry for it\n","    T_ = T.reset_index().drop(columns=['index'])\n","    for t_index in range(0, len(T)):\n","        # Make sure we aren't overwriting an entry!\n","        assert (T_.loc[t_index,r_on] not in T_map)\n","        T_map[T_.loc[t_index,r_on]] = T_.loc[t_index]\n","        count = count + 1\n","\n","    # Now find matches\n","    S_ = S.reset_index().drop(columns=['index'])\n","    for s_index in range(0, len(S)):\n","        count = count + 1\n","        if S_.loc[s_index, l_on] in T_map:\n","                ret = ret.append(S_.loc[s_index].append(T_map[S_.loc[s_index, l_on]].drop(labels=r_on)), ignore_index=True)\n","\n","    print('Merge compared %d tuples'%count)\n","    return ret"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","deletable":false,"editable":false,"id":"OEgL47PKa-4-","nbgrader":{"cell_type":"markdown","checksum":"238fed9d183a3a48dc274222385fb871","grade":false,"grade_id":"cell-284837d450db82b2","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Step 2.1: Find a good order of evaluation.\n","\n","The following function, `recommend_jobs_basic`, takes as  input `recs_df`, `names_df` and `last_job_df` and returns the name of each `person_1` and the most recent `title` and `org` of each `person_2`.  \n","\n","We will time how long it takes to execute `recommend_jobs_basic` using the ordering `recs_df`, `names_df` and `last_job_df`.\n","\n","Your task is to improve this time by changing the join ordering used in `recommend_jobs_basic`."]},{"cell_type":"code","metadata":{"id":"43P-QhhOa6Q0","colab_type":"code","colab":{}},"source":["def recommend_jobs_basic(recs_df, names_df, last_job_df):\n","    return merge(merge(recs_df,names_df,'person_1','person')[['family_name','given_name','person_1','person_2']],\n","        last_job_df,'person_2','person')[['family_name','given_name','person_2','org','title']].sort_values('family_name')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"71SEjOpZXaPx","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"mi13-NOetIKd","colab_type":"code","colab":{}},"source":["%%time\n","\n","recs_new_df = recommend_jobs_basic(recs_df, names_df, last_job_df)\n","\n","if(len(recs_new_df.columns) != 5):\n","    raise AssertionError('Wrong number of columns in recs_new_df')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wFNsRkZwsQIy","colab_type":"text"},"source":["Modify the function `recommend_jobs_basic` in the cell below. See if it is possible to improve the efficiency by changing the join ordering to reduce the number of comparisons made in the `merge` function. "]},{"cell_type":"code","metadata":{"id":"6K6Ui-5zt1yP","colab_type":"code","colab":{}},"source":["# TODO: modify the order of joins to reduce comparisons\n","\n","def recommend_jobs_basic_reordered(recs_df, names_df, last_job_df):\n","    # YOUR CODE HERE\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rn5FY4x4t1nw","colab_type":"code","colab":{}},"source":["%%time\n","\n","recs_new_df = recommend_jobs_basic_reordered(recs_df, names_df, last_job_df)\n","\n","if(len(recs_new_df.columns) != 5):\n","    raise AssertionError('Wrong number of columns in recs_new_df')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNhog9r95eI0","colab_type":"code","colab":{}},"source":["names_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qwgez7Sk5jtT","colab_type":"code","colab":{}},"source":["recs_df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cza7DlZK-Wl4","colab_type":"code","colab":{}},"source":["last_job_df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l9XTSDnKt2SV","colab_type":"text"},"source":["## Step 2.2: Perform selections early using `merge` and `merge_map`\n"," \n","Reimplement `recommend_jobs_basic` using the `merge` and `merge_map` functions instead of Pandas' merge. Try to find the **most efficient** way by also considering the ordering.  "]},{"cell_type":"code","metadata":{"id":"v1_qDWg1sPga","colab_type":"code","colab":{}},"source":["# TODO: Reimplement recommend jobs using our custom merge and merge_map functions\n","\n","def recommend_jobs_new(recs_df, names_df, last_job_df):\n","    # YOUR CODE HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","deletable":false,"editable":false,"id":"aHKleD_beO-R","nbgrader":{"cell_type":"code","checksum":"7650a2937ca3f86e70c62f6f92d57a53","grade":true,"grade_id":"2-1-san","locked":true,"points":2,"schema_version":3,"solution":false,"task":false},"colab":{}},"source":["# Sanity Check 2.1 - please do not modify or delete this cell!\n","\n","%%time\n","\n","recs_new_df = recommend_jobs_new(recs_df, names_df, last_job_df)\n","\n","if(len(recs_new_df.columns) != 5):\n","    raise AssertionError('Wrong number of columns in recs_new_df')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IZcAFEbTwDk-","colab_type":"text"},"source":["# Step 3. Query Optimization in Databases\n","\n","Relational databases optimize queries by performing selections (and projections) as early as possible, and finding a good join ordering. We will therefore implement the recommend_jobs function using SQLite and see if it is faster. \n","\n","Dataframes `names_df`, `rec_df` and `last_job_df` are already stored in database `linkedin.db` with table name `names`, `recs` and `lastjob`. "]},{"cell_type":"markdown","metadata":{"id":"WTdjxdNo40ud","colab_type":"text"},"source":["## Step 3.1 \n","In the cell below, implement the `recommend_jobs_basic` function in SQL. Since the query is very fast, we will run the query 100 times to get an accurate idea of the execution time."]},{"cell_type":"code","metadata":{"id":"2_7i2SMGcgzD","colab_type":"code","colab":{}},"source":["%%time\n","for i in range(0, 100):\n","    # YOUR CODE HERE\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOm77epe5aUW","colab_type":"text"},"source":["## Step 3.2\n","Altough the execution is pretty fast, we can also create indices to make it even faster. Use the syntax `CREATE INDEX I ON T(C)` to create index on the three tables `recs`, `names`, and `lastjob`. Replace `I` with the name of the index that you wish to use, `T` with the name of the table and `C` with the name of the column. \n","\n","If you need to change the indices, you must drop them first using the following syntax: \n","`conn.execute('drop index if exists I')`\n","where I is the name of the index to be dropped."]},{"cell_type":"code","metadata":{"id":"b9mXyYFW1mJ8","colab_type":"code","colab":{}},"source":["conn.execute('begin transaction')\n","\n","# YOUR CODE HERE\n","\n","\n","conn.execute('commit')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ln2-jtMF6-T-","colab_type":"text"},"source":["In the cell below, rerun the query that you defined in Step 3.1 100 times get a new timing. The database will now use the indices that you created if they are beneficial to the execution. \n","\n","Is the query faster? "]},{"cell_type":"code","metadata":{"id":"4fbeYoQz1mlh","colab_type":"code","colab":{}},"source":["%%time\n","for i in range(0, 100):\n","    # YOUR CODE HERE\n"],"execution_count":0,"outputs":[]}]}